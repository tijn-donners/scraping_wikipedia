{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c3ce15-0f2d-4987-96c5-bfcafde6c8e4",
   "metadata": {},
   "source": [
    "# Scraping from Wikipedia\n",
    "\n",
    "[The User-Agent Policy of the Wikimedia Foundation](https://foundation.wikimedia.org/wiki/Policy:Wikimedia_Foundation_User-Agent_Policy) states that requests should contain a descriptive user-agent header. Therefore the bot name and version are identified in the header as well as my domain name.\n",
    "\n",
    "The following code requests a Wikipedia page that contains a list of links to Wikipedia entries of 17th century Dutch painters. The code creates a list of links that we later use for webscraping.\n",
    "\n",
    "the following libraries are used:\n",
    "- pandas\n",
    "- requests\n",
    "- beautifulsoup\n",
    "- re\n",
    "- json\n",
    "- geopy / Nominatim\n",
    "\n",
    "## 1. Getting a list of links (skip to 1.1 to save time on the scraping process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4282aa6e-f5b8-414a-8097-6ae55508a227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://nl.wikipedia.org/wiki/Johannes_van_der_Aeck', 'https://nl.wikipedia.org/wiki/Evert_van_Aelst', 'https://nl.wikipedia.org/wiki/Willem_van_Aelst', 'https://nl.wikipedia.org/wiki/Jan_van_Aken_(1614-1661)', 'https://nl.wikipedia.org/wiki/Herman_van_Aldewereld']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "useragent = {                               #Create a user agent compliant to wikimedia policy\n",
    "    \"User-Agent\": (\"WikiScraper/1.0 (contact@tijndonners.nl) \" \n",
    "                   \"requests/2.32.3\")}\n",
    "\n",
    "response = requests.get(\"https://nl.wikipedia.org/wiki/Lijst_van_Nederlandse_kunstschilders\", headers= useragent).text  #This Dutch wikipedia page lists 17th century painters that have a wikipedia page \n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "h2_17 = soup.find(\"h2\", id=\"17e_eeuw\")\n",
    "\n",
    "links = []\n",
    "\n",
    "#Iterate through the elements in h2_17 (these contain all htmtl elements after the 17th century header)\n",
    "for el in h2_17.find_all_next():\n",
    "    # Stop when the next h2 (18th century) starts\n",
    "    if el.name == \"h2\":\n",
    "        break\n",
    "    \n",
    "    # Extract the link tags inside the element\n",
    "    if el.name == \"ul\":      #this leaves out figure elements that appear in between the person links\n",
    "        for a in el.find_all(\"a\", href=True):\n",
    "            href =  \"https:\" + a[\"href\"]    #a is a beautifulsoup tag object, ['href'] extracts the second part of the link\n",
    "            links.append(href)\n",
    "            \n",
    "print(links[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a930afc-0f85-4c0e-81e1-a47731670fc3",
   "metadata": {},
   "source": [
    "### Wikidata Links\n",
    "\n",
    "Since Wikidata entries contain more structured data than a wikipedia page does, we want to retrieve the wikidata links from the wikipedia links that was just created\n",
    "\n",
    "#### This takes a long time! (minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c4df653-0be0-4666-b9ab-7c1071221b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.wikidata.org/wiki/Special:EntityPage/Q1033616', 'https://www.wikidata.org/wiki/Special:EntityPage/Q759747', 'https://www.wikidata.org/wiki/Special:EntityPage/Q553273', 'https://www.wikidata.org/wiki/Special:EntityPage/Q6150343', 'https://www.wikidata.org/wiki/Special:EntityPage/Q1703946']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "wikidata_links =[]\n",
    "\n",
    "#this might take a while! It's processing a lot of requests\n",
    "for url in links:\n",
    "    r = BeautifulSoup(requests.get(url, headers= useragent).text, 'html.parser')\n",
    "    wd = r.select_one(\"li#t-wikibase a\") #identify the element that contains the wikidata link\n",
    "    if wd:\n",
    "        wikidata_links.append(wd[\"href\"]) #if there is a link, append it to the wikidata_links list  \n",
    "\n",
    "print(wikidata_links[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c07b1b-cebb-46f1-8635-efe01833cdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456\n",
      "468\n"
     ]
    }
   ],
   "source": [
    "print(len(wikidata_links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f6f83-e1d8-48fa-b1a3-cd8c9487c2d9",
   "metadata": {},
   "source": [
    "### To reduce request overhead and save time, we cache the wikidata_links and links variables in local JSON files for reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49f0cc0b-e5a5-4da1-8f4d-f55ae7424de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"wikidata_links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(wikidata_links, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "with open(\"links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(links, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ec38e-69b7-482b-8378-13d9f09ba039",
   "metadata": {},
   "source": [
    "## 1.1  Here you can import the results from all previous code without performing the lengthy request process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4447577f-2565-4659-90b7-93062d135191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    links = json.load(f)\n",
    "\n",
    "with open(\"wikidata_links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    wikidata_links = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e80eb-5b1c-47df-ab2b-1da58dfe280d",
   "metadata": {},
   "source": [
    "Some Painters do not have a wikipedia page yet, and therefore they should be removed from the list of links in order to have an equal amount of links in both lists, which is important for creating a DataFrame later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49ee3818-95a9-4ccd-8178-6c1f83cdd409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lists have equal length: 456 & 456\n"
     ]
    }
   ],
   "source": [
    "for link in links:\n",
    "    if 'redlink' in link:   #the word redlink identifies broken links!\n",
    "        links.remove(link)\n",
    "        \n",
    "if len(links) == len(wikidata_links):\n",
    "    print(f'lists have equal length: {len(links)} & {len(wikidata_links)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7d0fe-ee72-409b-a0fb-3a2dc67b3cce",
   "metadata": {},
   "source": [
    "## 2. Scraping data\n",
    "Now that we have all the wikidata links alongside the wikipedia links, we can scrape the articles itseld but also metadata like gender, place of birth and place of death on wikidata. We add this data to a pandas dataframe.\n",
    "\n",
    "**Please note that since this data concerns historical persons, not all data is available on wikipedia/wikidata. Therefore, missing values will be assigned 'na'.**\n",
    "\n",
    "## This takes a lot of time! Grab some coffee. (c. 8 minutes for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14c4e0-2de2-41ca-9e2e-1fd48d0260f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# initiate a dictionary for the scraped data\n",
    "results = {'Name':[],\n",
    "           'Wikidata Identifier':[],\n",
    "           'Wikipedia Article':[],\n",
    "           'Gender':[],\n",
    "           'Year of Birth':[],\n",
    "           'Year of Death':[],\n",
    "           'Place of Birth':[],\n",
    "           'Place of Death':[]} \n",
    "\n",
    "for url in links:\n",
    "    r = BeautifulSoup(requests.get(url, headers=useragent).text, \"html.parser\")\n",
    "    text = \" \".join(p.get_text() for p in r.find_all(\"p\"))                    #Note that headers are not included\n",
    "    results['Wikipedia Article'].append(text.replace('\\n', ' '))\n",
    "\n",
    "    \n",
    "for url in wikidata_links:\n",
    "    r = BeautifulSoup(requests.get(url, headers= useragent).text, 'html.parser')\n",
    "    \n",
    "    #scrape the title and wikidata item identifier\n",
    "    wikidata_item = r.find('h1').text.replace('\\n', ' ').strip().rsplit(\" \",1)\n",
    "    results['Wikidata Identifier'].append(wikidata_item[1].replace('(','').replace(')',''))\n",
    "    results['Name'].append(wikidata_item[0])\n",
    "\n",
    "    # For the rest of the data, try blocks are used since BeuatifulSoup raises errors when values are missing.\n",
    "    # If this happens, instead of the code breaking, 'na' will be added to to dictionary with results.\n",
    "    \n",
    "    # scrape birth year\n",
    "    try:\n",
    "        byear = r.find(\"div\", id=\"P569\").find(\"div\", attrs=\"wikibase-snakview-value wikibase-snakview-variation-valuesnak\").text\n",
    "        results['Year of Birth'].append(int(re.search(r\"\\d{4}\", byear).group(0)))  #extract only the 4 digit sequence and remove clutter\n",
    "    except Exception as e:\n",
    "        results['Year of Birth'].append('na')\n",
    "\n",
    "    # scrape death year\n",
    "    try:\n",
    "        dyear = r.find(\"div\", id=\"P570\").find(\"div\", attrs=\"wikibase-snakview-value wikibase-snakview-variation-valuesnak\").text\n",
    "        results['Year of Death'].append(int(re.search(r\"\\d{4}\", dyear).group(0)))\n",
    "    except Exception as e:\n",
    "        results['Year of Death'].append('na')\n",
    "    \n",
    "    # scrape gender\n",
    "    try:\n",
    "        gender = r.find(\"div\", id=\"P21\").find(\"div\", attrs=\"wikibase-snakview-value wikibase-snakview-variation-valuesnak\").text  #P21 is the wikidata property tag for sex or gender\n",
    "        results['Gender'].append(gender)\n",
    "    except Exception as e:\n",
    "        results['Gender'].append('na')\n",
    "\n",
    "    # scrape birth place\n",
    "    try:\n",
    "        bplace = r.find(\"div\", id=\"P19\").find(\"div\", attrs=\"wikibase-snakview-value wikibase-snakview-variation-valuesnak\").text\n",
    "        results['Place of Birth'].append(bplace)\n",
    "    except Exception as e:\n",
    "        results['Place of Birth'].append('na')\n",
    "\n",
    "    # scrape death place\n",
    "    try:\n",
    "        dplace = r.find(\"div\", id=\"P20\").find(\"div\", attrs=\"wikibase-snakview-value wikibase-snakview-variation-valuesnak\").text\n",
    "        results['Place of Death'].append(dplace)\n",
    "    except Exception as e:\n",
    "        results['Place of Death'].append('na')\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20928385-91a0-4dba-9121-6a4012c869c9",
   "metadata": {},
   "source": [
    "## 3. Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5213e7-b434-433f-a6a8-e0b61734a125",
   "metadata": {},
   "source": [
    "The wikipedia articles contain citations in this form: [1] and [2]. These will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a51b45eb-6f3b-4e8b-a97a-4ea2f7e3e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r\"\\[\\d+\\]\", '', text) #regex is used to replace citations with empty strings\n",
    "    \n",
    "cleaned_df = df.copy()\n",
    "\n",
    "cleaned_df['Wikipedia Article'] = cleaned_df['Wikipedia Article'].apply(clean_text) #note that this overwrites the original column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8c799-564a-42d1-8a70-2b6264a6b616",
   "metadata": {},
   "source": [
    "## 4. cleaning the CSV file\n",
    "After a manual check of the data, a few anomalies emerge. One way to check if there are rows that are not persons, is to check if the gender is NOT male or female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1490910a-4024-4655-9ba3-f35162d46aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Wikidata Identifier</th>\n",
       "      <th>Wikipedia Article</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>Year of Death</th>\n",
       "      <th>Place of Birth</th>\n",
       "      <th>Place of Death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Groningen</td>\n",
       "      <td>Q749</td>\n",
       "      <td>De Nederlandse stad Groningen uitspraakⓘ (Gron...</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Cambrai</td>\n",
       "      <td>Q181285</td>\n",
       "      <td>Cambrai (Nederlands, in historische context ge...</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Jan and Jacob Pynas</td>\n",
       "      <td>Q14632488</td>\n",
       "      <td>De broers Jan Symonsz Pynas (ca. 1583, Alkmaar...</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Jan and Jacob Pynas</td>\n",
       "      <td>Q14632488</td>\n",
       "      <td>De broers Jan Symonsz Pynas (ca. 1583, Alkmaar...</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name Wikidata Identifier  \\\n",
       "128            Groningen                Q749   \n",
       "263              Cambrai             Q181285   \n",
       "326  Jan and Jacob Pynas           Q14632488   \n",
       "327  Jan and Jacob Pynas           Q14632488   \n",
       "\n",
       "                                     Wikipedia Article Gender Year of Birth  \\\n",
       "128  De Nederlandse stad Groningen uitspraakⓘ (Gron...     na            na   \n",
       "263  Cambrai (Nederlands, in historische context ge...     na            na   \n",
       "326  De broers Jan Symonsz Pynas (ca. 1583, Alkmaar...     na            na   \n",
       "327  De broers Jan Symonsz Pynas (ca. 1583, Alkmaar...     na            na   \n",
       "\n",
       "    Year of Death Place of Birth Place of Death  \n",
       "128            na             na             na  \n",
       "263            na             na             na  \n",
       "326            na             na             na  \n",
       "327            na             na             na  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if data entry is a person:\n",
    "filtered_df = cleaned_df[cleaned_df['Gender'].isin(['na'])]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04a026-a576-403e-a6b3-f924ac1f9dcc",
   "metadata": {},
   "source": [
    "As we can see, Groningen en Cambrai (cities) and the brothers Jan and Jacob are showing up. Since there is no metadata available for the brothers, we remove them from the data alongside the two cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87e53b0c-ab34-42dc-a40d-ab5de345fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df[cleaned_df['Gender'].isin(['male','female'])] #filters out the unwanted rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5cbe6-b39e-43f5-8924-efcff8f3c506",
   "metadata": {},
   "source": [
    "#### Now we check for duplicates and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ab7fcc8-b270-4ee8-a3eb-53b33ba28c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wikidata Identifier\n",
       "Q339270     2\n",
       "Q2459649    2\n",
       "Q1854019    2\n",
       "Q2057521    2\n",
       "Q944834     2\n",
       "           ..\n",
       "Q864447     1\n",
       "Q2112735    1\n",
       "Q2462870    1\n",
       "Q2103594    1\n",
       "Q5529917    1\n",
       "Name: count, Length: 447, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['Wikidata Identifier'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad03e0-8b8a-481b-b596-fa33ae21b413",
   "metadata": {},
   "source": [
    "We can see some duplicate values!\n",
    "In the output above, we see **length: 447**, which means there are 447 unique wikidata ID's and therefore painters.\n",
    "In the code below, the duplicates are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57567cf5-2496-4ba8-a913-c795252f2c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = cleaned_df.drop_duplicates('Wikidata Identifier') #removes duplicate rows (keeps the first row by default, but this does not matter much in this case)\n",
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72ea4e-fbee-4a54-af18-f33c2179cda4",
   "metadata": {},
   "source": [
    "## 5. Adding coordinates\n",
    "In order to make some geographical visualizations, we need to geocode the birth and death places and add them to the dataframe.\n",
    "The coordinates are decimal degrees (WGS84, EPSG:4326) in (lat,lon) format.\n",
    "\n",
    "#### Once again, this takes a while (c. 16 minutes for me)\n",
    "The RateLimiter might catch some timeout errors in the output cell, but this will not affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a394352a-db64-4c7e-89a2-952f098f912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('Amsterdam',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/http/client.py\", line 1430, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/http/client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/http/client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/ssl.py\", line 1103, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 538, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Amsterdam&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Amsterdam&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Amsterdam&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('London',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/http/client.py\", line 1430, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/http/client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/http/client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/ssl.py\", line 1103, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 538, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 873, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=London&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=London&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tijn-do/anaconda3/lib/python3.12/site-packages/geopy/adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=London&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Wikidata Identifier</th>\n",
       "      <th>Wikipedia Article</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>Year of Death</th>\n",
       "      <th>Place of Birth</th>\n",
       "      <th>Place of Death</th>\n",
       "      <th>Birth Coordinates</th>\n",
       "      <th>Death Coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Johannes van der Aeck</td>\n",
       "      <td>Q1033616</td>\n",
       "      <td>Johannes Claesz. van der Aeck of Aack (gedoopt...</td>\n",
       "      <td>male</td>\n",
       "      <td>1636</td>\n",
       "      <td>1682</td>\n",
       "      <td>Leiden</td>\n",
       "      <td>Leiden</td>\n",
       "      <td>(52.1594747, 4.4908843)</td>\n",
       "      <td>(52.1594747, 4.4908843)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evert van Aelst</td>\n",
       "      <td>Q759747</td>\n",
       "      <td>Evert van Aelst (Delft, 1602 - aldaar, 19 febr...</td>\n",
       "      <td>male</td>\n",
       "      <td>1602</td>\n",
       "      <td>1657</td>\n",
       "      <td>Delft</td>\n",
       "      <td>Delft</td>\n",
       "      <td>(52.0114017, 4.35839)</td>\n",
       "      <td>(52.0114017, 4.35839)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Willem van Aelst</td>\n",
       "      <td>Q553273</td>\n",
       "      <td>Willem van Aelst of Guillermo van Aelst (Delft...</td>\n",
       "      <td>male</td>\n",
       "      <td>1627</td>\n",
       "      <td>1679</td>\n",
       "      <td>Delft</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>(52.0114017, 4.35839)</td>\n",
       "      <td>(52.3730796, 4.8924534)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan van Aken</td>\n",
       "      <td>Q6150343</td>\n",
       "      <td>Jan van Aken, ook Jean van Aken (Amsterdam, ca...</td>\n",
       "      <td>male</td>\n",
       "      <td>1614</td>\n",
       "      <td>1661</td>\n",
       "      <td>Kampen</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>(52.5559484, 5.9033303)</td>\n",
       "      <td>(52.3730796, 4.8924534)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Herman van Aldewereld</td>\n",
       "      <td>Q1703946</td>\n",
       "      <td>Herman van Aldewereld (1628/1629, Amsterdam – ...</td>\n",
       "      <td>male</td>\n",
       "      <td>1620</td>\n",
       "      <td>1669</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>(52.3730796, 4.8924534)</td>\n",
       "      <td>(52.3730796, 4.8924534)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name Wikidata Identifier  \\\n",
       "0  Johannes van der Aeck            Q1033616   \n",
       "1        Evert van Aelst             Q759747   \n",
       "2       Willem van Aelst             Q553273   \n",
       "3           Jan van Aken            Q6150343   \n",
       "4  Herman van Aldewereld            Q1703946   \n",
       "\n",
       "                                   Wikipedia Article Gender Year of Birth  \\\n",
       "0  Johannes Claesz. van der Aeck of Aack (gedoopt...   male          1636   \n",
       "1  Evert van Aelst (Delft, 1602 - aldaar, 19 febr...   male          1602   \n",
       "2  Willem van Aelst of Guillermo van Aelst (Delft...   male          1627   \n",
       "3  Jan van Aken, ook Jean van Aken (Amsterdam, ca...   male          1614   \n",
       "4  Herman van Aldewereld (1628/1629, Amsterdam – ...   male          1620   \n",
       "\n",
       "  Year of Death Place of Birth Place of Death        Birth Coordinates  \\\n",
       "0          1682         Leiden         Leiden  (52.1594747, 4.4908843)   \n",
       "1          1657          Delft          Delft    (52.0114017, 4.35839)   \n",
       "2          1679          Delft      Amsterdam    (52.0114017, 4.35839)   \n",
       "3          1661         Kampen      Amsterdam  (52.5559484, 5.9033303)   \n",
       "4          1669      Amsterdam      Amsterdam  (52.3730796, 4.8924534)   \n",
       "\n",
       "         Death Coordinates  \n",
       "0  (52.1594747, 4.4908843)  \n",
       "1    (52.0114017, 4.35839)  \n",
       "2  (52.3730796, 4.8924534)  \n",
       "3  (52.3730796, 4.8924534)  \n",
       "4  (52.3730796, 4.8924534)  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "df = cleaned_df.copy() \n",
    "\n",
    "# load geocoder\n",
    "geolocator = Nominatim(user_agent=\"tijn_d_scraper\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1) #limiting rates for OpenStreetMaps\n",
    "\n",
    "# geocode function\n",
    "def get_coords(place):\n",
    "    try:\n",
    "        location = geocode(place)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return \"na\"\n",
    "    except:\n",
    "        return \"na\"\n",
    "\n",
    "df['Birth Coordinates'] = df['Place of Birth'].apply(get_coords)\n",
    "df['Death Coordinates'] = df['Place of Death'].apply(get_coords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228dadf7-21e2-431b-b4a3-d97fe9af6714",
   "metadata": {},
   "source": [
    "### Dutch Painters from Namibia?\n",
    "\n",
    "For the 'na' values for place of birth/death, Nominatim returned the coordinates for Namibia, probably because 'na' was recognized as a country code. Naturally, we want to replace the coordinates of Namibia(-23.2335499, 17.3231107) with 'na'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08225973-fd72-4ebb-ba27-d06c1caf0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_cleaning(coords):\n",
    "    if coords == (-23.2335499, 17.3231107):\n",
    "        return 'na'\n",
    "    else:\n",
    "        return coords\n",
    "\n",
    "df['Birth Coordinates'] = df['Birth Coordinates'].apply(na_cleaning)\n",
    "df['Death Coordinates'] = df['Death Coordinates'].apply(na_cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10020318-a999-43d5-a363-01abd0a638b1",
   "metadata": {},
   "source": [
    "#### Unfortunately, Geopy has assigned slightly different cooirdinates for some places (like The Hague and Haarlem), so we have to normalize them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b988b7d2-7f10-4177-a781-6f73a54ca42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birth Coordinates\n",
       "(52.3730796, 4.8924534)    69\n",
       "(52.3885317, 4.6388048)    62\n",
       "(52.0799838, 4.3113461)    32\n",
       "(52.1594747, 4.4908843)    29\n",
       "(52.0114017, 4.35839)      27\n",
       "                           ..\n",
       "(51.782164, 5.1898267)      1\n",
       "(52.0373934, 4.3225028)     1\n",
       "(50.6450944, 5.5736112)     1\n",
       "(51.5725501, 8.1061259)     1\n",
       "(50.6365654, 3.0635282)     1\n",
       "Name: count, Length: 84, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_coords(df, place_col, coord_col):\n",
    "    # create a dictionary of place -> first coordinate found\n",
    "    mapping = df.groupby(place_col)[coord_col].first()  #it grabs the first coordinate it encounters and maps it the place name\n",
    "    # apply mapping to column\n",
    "    df[coord_col] = df[place_col].map(mapping)\n",
    "\n",
    "# applying the function\n",
    "normalize_coords(df, 'Place of Birth', 'Birth Coordinates')\n",
    "normalize_coords(df, 'Place of Death', 'Death Coordinates')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383588c1-357e-45bf-b86a-268e38fa751f",
   "metadata": {},
   "source": [
    "### Finally, we save the dataframe as a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee7fb9d-976f-4145-ad87-e490352b1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b0611-3c98-401e-9e6b-6450c90a393a",
   "metadata": {},
   "source": [
    "**Continue to the the folium_analysis.ipynb notebook in this repo to see georgraphical visualizations and analysis of the collected data.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
